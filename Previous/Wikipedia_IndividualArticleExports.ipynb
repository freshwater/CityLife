{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Util."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pyspark as ps\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "response_cache = {}\n",
    "\n",
    "def http_get(url):\n",
    "    if response := response_cache.get(url):\n",
    "        return response\n",
    "    else:\n",
    "        response = requests.request(url=url, method=\"GET\")\n",
    "        counties_list_html = response.content\n",
    "        response_cache[url] = counties_list_html\n",
    "        \n",
    "        return response_cache[url]\n",
    "\n",
    "spark = (ps.sql.SparkSession\n",
    "         .builder\n",
    "         .master('local[8]')\n",
    "         .appName('lecture')\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_export_get(title):\n",
    "    xml = http_get(\"https://en.wikipedia.org/wiki/Special:Export/\" + title)\n",
    "    \n",
    "    if b'<redirect title' in xml:\n",
    "        return wikipedia_export_get(title=BeautifulSoup(xml).select('redirect')[0]['title'])\n",
    "    else:\n",
    "        return xml\n",
    "\n",
    "def wikipedia_standard_url(title):\n",
    "    return \"https://en.wikipedia.org/wiki/\" + title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l data/communities_all.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as types\n",
    "\n",
    "schema = types.StructType([\n",
    "    types.StructField(name='id', dataType=types.LongType(), nullable=False),\n",
    "    types.StructField(name='Name', dataType=types.StringType(), nullable=False),\n",
    "    types.StructField(name='XML', dataType=types.StringType(), nullable=False) \n",
    "])\n",
    "\n",
    "data_frame = spark.createDataFrame([(hash(\"a\"), \"1ðŸ˜±2\", b\"c\".decode())], schema=schema)\n",
    "\n",
    "data_frame.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cities_titles = (spark.read.text('data/communities_all.txt').rdd\n",
    "                      .map(lambda row: row.value)\n",
    "                      .cache())\n",
    "\n",
    "cities_titles.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Wikitext Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stored Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_stored = spark.read.option('schema', schema).parquet(\"data/union13\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_new_count = 1400\n",
    "\n",
    "current_titles = data_frame_stored.rdd.map(lambda row: row[1])\n",
    "cities_remaining_titles = cities_titles.subtract(current_titles)\n",
    "print(\"current\", current_titles.count())\n",
    "print(\"total\", cities_titles.count())\n",
    "print(\"remaining\", cities_remaining_titles.count())\n",
    "\n",
    "cities_remaining_titles = cities_remaining_titles.sample(fraction=load_new_count/cities_remaining_titles.count(), withReplacement=False).cache()\n",
    "print(\"remaining sampled\", cities_remaining_titles.count())\n",
    "\n",
    "cities_exports = cities_remaining_titles.map(wikipedia_export_get).cache()\n",
    "cities_exports_keyed = cities_remaining_titles.map(hash).zip(cities_remaining_titles).zip(cities_exports)\n",
    "cities_proper_exports_keyed = cities_exports_keyed.filter(lambda keys_export: len(BeautifulSoup(keys_export[1]).select('text')) > 0)\n",
    "\n",
    "data_frame_new = spark.createDataFrame(cities_proper_exports_keyed.map(lambda keys_export: (keys_export[0][0], keys_export[0][1], keys_export[1].decode())), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_stored.intersect(data_frame_new).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_union = data_frame_stored.union(data_frame_new)\n",
    "# data_frame_union.count(), data_frame_union.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Write to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_frame_union.distinct().write.save(path=\"data/union16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_stored = data_frame_union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Newtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_infobox_key_values(text):\n",
    "    \"\"\"\n",
    "    {{Infobox settlement\n",
    "    | name = Dooley\n",
    "    | settlement_type = [[Ghost town]]\n",
    "    | image_skyline = \n",
    "    | imagesize =\n",
    "    | image_caption = \n",
    "    | pushpin_map = Montana#USA\n",
    "    | pushpin_label_position = left\n",
    "    | map_caption = Location of Dooley in Montana\n",
    "    | coordinates_footnotes = <ref>{{cite gnis |id=770722 |name=Dooley}}</ref>\n",
    "    | subdivision_type = [[List of sovereign states|Country]]\n",
    "    | subdivision_name = United States\n",
    "    | subdivision_type1 = [[U.S. state|State]]\n",
    "    | subdivision_name1 = [[Montana]]\n",
    "    | subdivision_type2 = [[List of counties in Montana|County]]\n",
    "    | subdivision_name2 = [[Sheridan County, Montana|Sheridan]]\n",
    "    | established_title = Established\n",
    "    | established_date = 1913\n",
    "    | named_for = \n",
    "    | extinct_title = Abandoned\n",
    "    | extinct_date = 1957\n",
    "    | elevation_ft = 2461\n",
    "    | coordinates = {{coord|48|52|52|N|104|23|22|W|region:US-MT|display=inline,title}}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    regex = r\"^\\s*\\|\\s*(.+?)\\s*=(\\s*|(\\s*(.*)\\s*))$\"\n",
    "    key_values = re.findall(regex, text, re.MULTILINE)\n",
    "\n",
    "    return [(key, value) for key, _, _, value in key_values]\n",
    "\n",
    "def wikipedia_infobox_key_values_non_empty(text):\n",
    "    return [(key, value) for key, value in wikipedia_infobox_key_values(text) if value != '']\n",
    "\n",
    "wikipedia_infobox_key_values(\"abc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (data_frame_stored.rdd # .sample(fraction=50/data_frame_stored.count(), withReplacement=False)\n",
    "                           .map(lambda row: BeautifulSoup(row.XML).select('text')[0].getText())\n",
    "                           .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['mining', 'ghost', 'awesome', 'death', 'taxes', 'high tax', 'low tax',\n",
    "                'superhero', 'batman', ' men', ' women', 'spongebob', 'corporations']\n",
    "\n",
    "search_terms += ['oil', ' oil', 'oil ', ' oil ', 'oil.', '. oil']\n",
    "\n",
    "search_terms += ['gazette', 'journal', 'wall street', 'railroad', 'baron',\n",
    "                 'polish', 'poland', 'french', 'france', 'film', 'movies', ' media ',\n",
    "                 'technology', 'farm', 'farming', 'orchard', 'automotive', 'entertainment',\n",
    "                 'immigrants', 'immigration', 'freelance', 'volcan',\n",
    "                 'eruption', 'flood', 'tornado', 'hurricane', 'katrina', 'hurricane katrina']\n",
    "\n",
    "# search_terms = map(str, range(1200, 2080))\n",
    "\n",
    "dict_ = {}\n",
    "for search_term in search_terms:\n",
    "    samples_filtered = sample.filter(lambda text: search_term.lower() in text.lower())\n",
    "    dict_[search_term] = samples_filtered.count()\n",
    "    print(f'{search_term}\\t\\t{samples_filtered.count() / sample.count():0.5f} {samples_filtered.count()}/{sample.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = np.array([int(key) for key, value in date_frequencies.items()])\n",
    "ys = np.array([value for key, value in date_frequencies.items()])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.fill_between(xs, y1=0, y2=ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_infobox = (sample.map(wikipedia_infobox_key_values_non_empty)\n",
    "                         .map(lambda key_values: [key for key, value in key_values])\n",
    "                         .map(set).reduce(set.union))\n",
    "\n",
    "len(samples_infobox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox_keys_counts = (sample.map(wikipedia_infobox_key_values_non_empty)\n",
    "                         .flatMap(lambda key_values: [key for key, value in key_values])\n",
    "                         .countByValue())\n",
    "                         \n",
    "infobox_keys_counts = [key_count for key_count in infobox_keys_counts.items() if key_count[1] > 20]\n",
    "sorted(infobox_keys_counts, key=lambda kv: -kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = (data_frame_stored.rdd # .sample(fraction=50/data_frame_stored.count(), withReplacement=False)\n",
    "                               .map(lambda row: BeautifulSoup(row.XML).select('text')[0].getText())\n",
    "                               .cache())\n",
    "\n",
    "text = sample.take(1)[0]\n",
    "\n",
    "wikipedia_infobox_key_values_non_empty(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = BeautifulSoup(xml).select('text')[0].getText()\n",
    "\n",
    "# wikipedia_infobox_key_values(text)\n",
    "wikipedia_infobox_key_values_non_empty(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Potential inquiries:\n",
    " \n",
    " Nearness to rivers/lakes/roads  \n",
    " Mining  \n",
    " Text quantity as rough measure of significance  \n",
    " Dates extract to estimate approximate time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = spark.read.option('schema', schema).parquet(\"union9\").rdd\n",
    "data_frame.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spooky = (data_frame.filter(lambda row: 'ghost' in row.XML.lower())\n",
    "                    .map(lambda row: row.XML)\n",
    "                    .cache())\n",
    "\n",
    "print(spooky.count())\n",
    "print(BeautifulSoup(spooky.sample(fraction=0.5, withReplacement=False).take(20)[8]).select('text')[0].getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import urllib\n",
    "\n",
    "\n",
    "def wikipedia_file_url(file):\n",
    "    # return \"https://upload.wikimedia.org/wikipedia/commons/8/81/\" + urllib.parse.quote(file, safe=' ').replace(' ', '_')\n",
    "    return \"https://en.wikipedia.org/wiki/Special:FilePath/\" + file\n",
    "\n",
    "file = 'Bellfonte, Alabama- The Chimney of the Local Inn.JPG'\n",
    "file = 'Reno skyline.JPG'\n",
    "file = 'Cape dec29-07 (23).JPG'\n",
    "file = 'Abandoned school in Toyah, Texas.jpg'\n",
    "HTML(f'<img src=\"{wikipedia_file_url(file)}\" />') # , wikipedia_file_url(file), 'https://upload.wikimedia.org/wikipedia/commons/8/81/Bellfonte%2C_Alabama-_The_Chimney_of_the_Local_Inn.JPG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BeautifulSoup(spooky.sample(fraction=0.05, withReplacement=False).take(1)[0]).select('text')[0].getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|settlement_type          = [[Ghost town]]\n",
    "|settlement_type          = [[Ghost town|Ghost Town]]\n",
    "|coordinates              = {{coord|34|42|40|N|85|56|43|W|region:US-AL|display=inline,title}}\n",
    "|coordinates   = {{coord|31|15|21|N|91|36|30|W|region:US-MS|display=inline,title}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[Category:Former populated places in Minnesota]]\n",
    "[[Category:Former populated places in Rock County, Minnesota]]\n",
    "[[Category:Ghost towns in Alabama]]\n",
    "[[Category:Ghost towns in West Texas]]\n",
    "[[Category:Ghost towns in Nye County, Nevada]]\n",
    "[[Category:Ghost towns in Nevada]]\n",
    "[[Category:Former populated places in Adams County, Mississippi]]\n",
    "[[Category:Former populated places in Mississippi]]\n",
    "[[Category:Former populated places in Oregon]]\n",
    "[[Category:Destroyed cities]]\n",
    "{{US-ghost-town-stub}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Bellefonte''' is a [[ghost town]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "==Demographics==\n",
    "{{US Census population\n",
    "|1830= 1414\n",
    "|1840= 1743\n",
    "|1850= 1761\n",
    "|1860= 1942\n",
    "|1870= 1704\n",
    "|1880= 1767\n",
    "|1890= 1728\n",
    "|1900= 1548\n",
    "|1910= 1644\n",
    "|1920= 1518\n",
    "|1930= 1518\n",
    "|1940= 1700\n",
    "|1950= 1778\n",
    "|1960= 2145\n",
    "|1970= 2505\n",
    "|1980= 2792\n",
    "|1990= 2667\n",
    "|2000= 2692\n",
    "|2010= 2775\n",
    "|estyear=2016\n",
    "|estimate=2783\n",
    "|estref=<ref name=\"USCensusEst2016\">{{cite web|url=https://www.census.gov/programs-surveys/popest/data/tables.2016.html|title=Population and Housing Unit Estimates|accessdate=June 9, 2017}}</ref>\n",
    "|footnote=U.S. Decennial Census<ref name=\"DecennialCensus\">{{cite web|url=https://www.census.gov/programs-surveys/decennial-census.html|title=Census of Population and Housing|publisher=Census.gov|accessdate=June 4, 2015}}</ref>\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "&lt;!-- Population------------------&gt;\n",
    "| population_total        = 320\n",
    "| population_as_of        = 2010\n",
    "| population_density_km2  = auto\n",
    "|population_metro         = \n",
    "\n",
    "&lt;!-- Population --&gt;\n",
    "|population_as_of         = \n",
    "|population_footnotes     =\n",
    "|population_total         = \n",
    "|population_density_km2   = \n",
    "|population_density_sq_mi =\n",
    "\n",
    "&lt;!-- postal codes, area code --&gt;\n",
    "| geocode                 = \n",
    "| iso_code                = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
