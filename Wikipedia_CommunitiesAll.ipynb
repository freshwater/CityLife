{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker run -d --name sparkbook -p 8881:8888 -v \"$PWD\":/home/jovyan/work jupyter/pyspark-notebook start.sh jupyter lab --LabApp.token=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m5.8xlarge  128GiB  \n",
    "r5.4xlarge  128GiB  \n",
    "x1e.xlarge  122GiB  \n",
    "x1.16xlarge 976GiB  \n",
    "i3.4xlarge  122GiB  \n",
    "m5n.4xlarge\t64 GiB UT25Gbps  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pyspark as ps\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "response_cache = {}\n",
    "\n",
    "def http_get(url):\n",
    "    if response := response_cache.get(url):\n",
    "        return response\n",
    "    else:\n",
    "        response = requests.request(url=url, method=\"GET\")\n",
    "        counties_list_html = response.content\n",
    "        response_cache[url] = counties_list_html\n",
    "        \n",
    "        return response_cache[url]\n",
    "\n",
    "spark = (ps.sql.SparkSession\n",
    "         .builder\n",
    "         .master('local[8]')\n",
    "         .appName('lecture')\n",
    "         .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Holmes_County,_Ohio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_list_url = \"https://en.wikipedia.org/wiki/List_of_United_States_counties_and_county_equivalents\"\n",
    "\n",
    "def wikipedia_export_get(title):\n",
    "    xml = http_get(\"https://en.wikipedia.org/wiki/Special:Export/\" + title)\n",
    "    \n",
    "    if b'<redirect title' in xml:\n",
    "        return wikipedia_export_get(title=BeautifulSoup(xml).select('redirect')[0]['title'])\n",
    "    else:\n",
    "        return xml\n",
    "\n",
    "def wikipedia_standard_url(title):\n",
    "    return \"https://en.wikipedia.org/wiki/\" + title\n",
    "\n",
    "# def wikipedia_standard_url_encode(title):\n",
    "#     import urllib.parse\n",
    "#     return \"https://en.wikipedia.org/wiki/\" + urllib.parse.quote(title)\n",
    "\n",
    "def wikipedia_counties_titles():\n",
    "    soup = BeautifulSoup(http_get(counties_list_url))\n",
    "    print(soup.select('.wikitable.sortable caption big'))\n",
    "    \n",
    "    rows = soup.select('.wikitable.sortable tbody tr')\n",
    "    anchors = sum([row.select('td a')[:1] for row in rows], [])\n",
    "\n",
    "    urls = [a['href'][len('/wiki/'):] for a in anchors]\n",
    "    \n",
    "    return urls\n",
    "    \n",
    "def wikipedia_communities_subheadings(text):\n",
    "    import re\n",
    "\n",
    "    regex = r\"(^==\\s?Communities\\s?==)(.+?)(^==[^=].+?[^=]==)\"\n",
    "    post_communities = re.findall(regex, text, re.MULTILINE + re.DOTALL)\n",
    "    \n",
    "    if len(post_communities) == 0:\n",
    "        return []\n",
    "\n",
    "    # print(post_communities[0])\n",
    "    post_communities = post_communities[0]\n",
    "    \n",
    "    regex = r\"===(.+)===\"\n",
    "    res = [result.strip() for result in re.findall(regex, post_communities[1])]\n",
    "\n",
    "    return res\n",
    "\n",
    "time1 = time.time()\n",
    "wikipedia_counties_titles()\n",
    "f'time.{time.time() - time1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wikipedia_communities_extract(text):\n",
    "    import re\n",
    "\n",
    "    regex = r\"(^==\\s?Communities\\s?==)(.+?)(^==[^=].+?[^=]==)\"\n",
    "    post_communities = re.findall(regex, text, re.MULTILINE + re.DOTALL)\n",
    "    \n",
    "    regex = r\"(^==\\s?Municipalities\\s?==)(.+?)(^==[^=].+?[^=]==)\"\n",
    "    municipalities = re.findall(regex, text, re.MULTILINE + re.DOTALL)\n",
    "    \n",
    "    regex = r\"(^==\\s?Cities and communities\\s?==)(.+?)(^==[^=].+?[^=]==)\"\n",
    "    cities_and_communities = re.findall(regex, text, re.MULTILINE + re.DOTALL)\n",
    "    \n",
    "    regex = r\"(^==\\s?Community\\s?==)(.+?)(^==[^=].+?[^=]==)\"\n",
    "    community = re.findall(regex, text, re.MULTILINE + re.DOTALL)\n",
    "    \n",
    "    types = []\n",
    "    if len(post_communities) > 0:\n",
    "        types.append(\"Communities\")\n",
    "        \n",
    "    if len(municipalities) > 0:\n",
    "        types.append(\"Municipalities\")\n",
    "        if len(municipalities) > len(post_communities):\n",
    "            post_communities = municipalities\n",
    "\n",
    "    if len(cities_and_communities) > 0:\n",
    "        types.append(\"Cities and communities\")\n",
    "        if len(cities_and_communities) > len(post_communities):\n",
    "            post_communities = cities_and_communities\n",
    "\n",
    "    if len(community) > 0:\n",
    "        types.append(\"Community\")\n",
    "        if len(community) > len(post_communities):\n",
    "            post_communities = community\n",
    "    \n",
    "    if len(post_communities) == 0:\n",
    "        return []\n",
    "\n",
    "    post_communities = post_communities[0]\n",
    "    \n",
    "    regex = r\"===(.+)===\"\n",
    "    communities_subheadings = [result.strip() for result in re.findall(regex, post_communities[1])]\n",
    "    \n",
    "    ## --\n",
    "    \n",
    "    # print(re.findall(r\"\\*\\s*\\[\", post_communities[1]))\n",
    "    regex = r\"\\*\\s*\\[\\[(.+?),(.+?)\\|(.+?)\\]\\]\"\n",
    "    communities = re.findall(regex, post_communities[1])\n",
    "    \n",
    "    regex = r\"\\*\\s*\\[\\[(.+?)\\|(.+?)\\]\\]\"\n",
    "    communities_all = re.findall(regex, post_communities[1])\n",
    "    \n",
    "    regex = r\"\\*\\s*\\[\\[([^|]+?)\\]\\]\"\n",
    "    communities_untitled = re.findall(regex, post_communities[1])\n",
    "        \n",
    "    a = set([c[2] for c in communities])\n",
    "    b = set([c[1] for c in communities_all])\n",
    "    difference = b.difference(a).union(a.difference(b))\n",
    "    \n",
    "    return communities_subheadings, [c[0] for c in communities_all], difference, communities_untitled, types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = 'Garvin_County,_Oklahoma'\n",
    "print(wikipedia_standard_url(title))\n",
    "text = BeautifulSoup(wikipedia_export_get(title)).select('text')[0].getText()\n",
    "# print(text)\n",
    "wikipedia_communities_extract(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_export_get(twenty[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "twenty = wikipedia_counties_titles()\n",
    "\n",
    "counties_export_text = (sc.parallelize(twenty)\n",
    "                          .map(wikipedia_export_get)\n",
    "                          .map(lambda export: BeautifulSoup(export).select('text')[0].getText())\n",
    "                          .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_all = (counties_export_text.map(wikipedia_communities_extract)\n",
    "                                       .filter(lambda x: x != [])\n",
    "                                       .map(lambda x: x[1]))\n",
    "\n",
    "# out = [o for o in out if o[0] == tuple() and ',_Indiana' in o[1]]\n",
    "# out = [o for o in out if o[0] != tuple() and ',_Puerto_Rico' not in o[1]]\n",
    "# out = [o for o in out if o[0] != tuple() and ',_Indiana' in o[1]]\n",
    "# out = [o for o in out if o[0] != tuple() and ',_Virginia' in o[1]]\n",
    "# out = [o for o in out if o[0] != tuple() and ',_Alaska' in o[1]]\n",
    "\n",
    "communities_all_1 = list(communities_all.map(set).reduce(set.union))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(communities_all_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('communities_all.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(communities_all_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<br>'.join([f'<a href=\"{wikipedia_standard_url(title)}\">{title}</a>' for title in np.random.choice(communities_all_1, 250)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Newtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BeautifulSoup(wikipedia_export_get(np.random.choice(communities_all_1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
