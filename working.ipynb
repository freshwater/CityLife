{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 31.6 s, sys: 4.03 s, total: 35.6 s\nWall time: 37.1 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('data/enwiki-latest-pages-articles10.xml-p2336423p3046512')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((21720, 308503), 21719, 308503)"
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "pages = root.findall('./ns:page/ns:revision/ns:text', namespaces={'ns': 'http://www.mediawiki.org/xml/export-0.10/'});\n",
    "titles = root.findall('./ns:page/ns:title', namespaces={'ns': 'http://www.mediawiki.org/xml/export-0.10/'});\n",
    "\n",
    "assert len(titles) == len(pages), 1\n",
    "\n",
    "titles_texts = [(title.text, page.text) for title, page in zip(titles, pages) if '{{coord|' in (page.text or '')]\n",
    "len(titles_texts), len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Antarctica', 'Prince Charles Mountains'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_normalize(N, W):\n",
    "    degrees_N, minutes_N, seconds_N = N\n",
    "    degrees_W, minutes_W, seconds_W = W\n",
    "    \n",
    "    return (degrees_N + minutes_N / 60 + seconds_N / 3600,\n",
    "            degrees_W + minutes_W / 60 + seconds_W / 3600)\n",
    "            \n",
    "def coordinate_extract(coordinate_text):\n",
    "    import re\n",
    "    \n",
    "    regex = r\"{{coord\\|(.+?)\\|(.+?)\\|(.+?)\\|N\\|(.+?)\\|(.+?)\\|(.+?)\\|W(?:\\|.+)?\"\n",
    "    results = re.findall(regex, coordinate_text, re.IGNORECASE)\n",
    "\n",
    "    if results != []:\n",
    "        # print(results)\n",
    "        dn, mn, sn, dw, mw, sw = [float(num) for num in results[0]]\n",
    "        coordinate = coordinate_normalize(N=(dn, mn, sn), W=(-dw, -mw, -sw))\n",
    "        return coordinate\n",
    "    else:\n",
    "        regex = r\"{{coord\\|(.+?)\\|(.+?)\\|N\\|(.+?)\\|(.+?)\\|W(?:\\|.+)?\"\n",
    "        results = re.findall(regex, coordinate_text, re.IGNORECASE)\n",
    "        \n",
    "    if results != [] and '|' not in results[0]:\n",
    "        dn, mn, dw, mw = [float(num) for num in results[0]]\n",
    "        coordinate = coordinate_normalize(N=(dn, mn, 0), W=(-dw, -mw, 0))\n",
    "        \n",
    "        return coordinate\n",
    "    else:\n",
    "        regex = r\"{{coord\\|(.+?)\\|+N\\|(.+?)\\|+W(?:\\|.+)?\"\n",
    "        results = re.findall(regex, coordinate_text, re.IGNORECASE)\n",
    "        \n",
    "    if results != []:\n",
    "        dn, dw = [float(num) for num in results[0]]\n",
    "        coordinate = coordinate_normalize(N=(dn, 0, 0), W=(-dw, 0, 0))\n",
    "    else:\n",
    "        regex = r\"{{coord\\|(.+?)\\|([.\\-0-9]+)\"\n",
    "        results = re.findall(regex, coordinate_text, re.IGNORECASE)\n",
    "        \n",
    "    if results != []:\n",
    "        dn, dw = [float(num) for num in results[0]]\n",
    "        coordinate = coordinate_normalize(N=(dn, 0, 0), W=(dw, 0, 0))\n",
    "        \n",
    "        return coordinate\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def coordinate_try(coordinate_text):\n",
    "    try:\n",
    "        return coordinate_extract(coordinate_text)\n",
    "    except:\n",
    "        # print(\"E:\" + coordinate_text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "31467 30611\n"
    }
   ],
   "source": [
    "import re\n",
    "def coordinates_parse(text):\n",
    "    return re.findall(r\"{{coord\\|.+?}}\", text)\n",
    "\n",
    "coordinates_all = sum([coordinates_parse(text) for title, text in titles_texts], [])\n",
    "# coordinates_all = [coordinate_data for coordinate_text in coordinates_all\n",
    "#                    if coordinate_data := coordinate_try(coordinate_text)]\n",
    "\n",
    "print(len(coordinates_all), len(set(coordinates_all)))\n",
    "coordinates_all = [coordinate_try(coordinate_text) for coordinate_text in coordinates_all if coordinate_try(coordinate_text)]\n",
    "\n",
    "len(coordinates_all), len(set(coordinates_all))\n",
    "\n",
    "coordinates_all = set(coordinates_all)\n",
    "\n",
    "coordinates_master_list = list(coordinates_all)\n",
    "coordinates_indices_dict = {coordinate: index for index, coordinate in enumerate(coordinates_master_list)}\n",
    "\n",
    "def coordinate_index(coordinate_text):\n",
    "    if coordinate_text:\n",
    "        coordinate = coordinate_try(coordinate_text)\n",
    "\n",
    "        if coordinate:\n",
    "            return coordinates_indices_dict[coordinate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates_all = sum([[(coordinate, index) for coordinate in coordinates_parse(text)]\n",
    "                       for index, (title, text) in enumerate(titles_texts)], [])\n",
    "\n",
    "coordinates_articles_index_pairs = [(coordinate_index(coordinate_text), index)\n",
    "                                    for coordinate_text, index in coordinates_all if coordinate_try(coordinate_text)]\n",
    "\n",
    "coordinates_articles_index_array = list(map(lambda _: set(), range(len(coordinates_master_list))))\n",
    "for coordinate_index_, article_index in coordinates_articles_index_pairs:\n",
    "    coordinates_articles_index_array[coordinate_index_].add(article_index)\n",
    "\n",
    "with open('database/coordinates_articles_index_array.json', 'w') as file:\n",
    "    file.write(json.dumps(list(map(list, coordinates_articles_index_array))))\n",
    "\n",
    "titles_all = [title for title, text in titles_texts]\n",
    "\n",
    "with open('database/article_titles_array.json', 'w') as file:\n",
    "    file.write(json.dumps(titles_all, separators=(',', ':')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "alphabet: a b c d e f g h i j k l m n o p q r s t u v w x y z\nCPU times: user 1min 49s, sys: 9.36 s, total: 1min 58s\nWall time: 2min 7s\n"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# https://dev.mysql.com/doc/refman/5.7/en/fulltext-stopwords.html#fulltext-stopwords-stopwords-for-myisam-search-indexes\n",
    "\n",
    "stopwords = \"\"\"\n",
    "a's           able          about         above         according\n",
    "accordingly   across        actually      after         afterwards\n",
    "again         against       ain't         all           allow\n",
    "allows        almost        alone         along         already\n",
    "also          although      always        am            among\n",
    "amongst       an            and           another       any\n",
    "anybody       anyhow        anyone        anything      anyway\n",
    "anyways       anywhere      apart         appear        appreciate\n",
    "appropriate   are           aren't        around        as\n",
    "aside         ask           asking        associated    at\n",
    "available     away          awfully       be            became\n",
    "because       become        becomes       becoming      been\n",
    "before        beforehand    behind        being         believe\n",
    "below         beside        besides       best          better\n",
    "between       beyond        both          brief         but\n",
    "by            c'mon         c's           came          can\n",
    "can't         cannot        cant          cause         causes\n",
    "certain       certainly     changes       clearly       co\n",
    "com           come          comes         concerning    consequently\n",
    "consider      considering   contain       containing    contains\n",
    "corresponding could         couldn't      course        currently\n",
    "definitely    described     despite       did           didn't\n",
    "different     do            does          doesn't       doing\n",
    "don't         done          down          downwards     during\n",
    "each          edu           eg            eight         either\n",
    "else          elsewhere     enough        entirely      especially\n",
    "et            etc           even          ever          every\n",
    "everybody     everyone      everything    everywhere    ex\n",
    "exactly       example       except        far           few\n",
    "fifth         first         five          followed      following\n",
    "follows       for           former        formerly      forth\n",
    "four          from          further       furthermore   get\n",
    "gets          getting       given         gives         go\n",
    "goes          going         gone          got           gotten\n",
    "greetings     had           hadn't        happens       hardly\n",
    "has           hasn't        have          haven't       having\n",
    "he            he's          hello         help          hence\n",
    "her           here          here's        hereafter     hereby\n",
    "herein        hereupon      hers          herself       hi\n",
    "him           himself       his           hither        hopefully\n",
    "how           howbeit       however       i'd           i'll\n",
    "i'm           i've          ie            if            ignored\n",
    "immediate     in            inasmuch      inc           indeed\n",
    "indicate      indicated     indicates     inner         insofar\n",
    "instead       into          inward        is            isn't\n",
    "it            it'd          it'll         it's          its\n",
    "itself        just          keep          keeps         kept\n",
    "know          known         knows         last          lately\n",
    "later         latter        latterly      least         less\n",
    "lest          let           let's         like          liked\n",
    "likely        little        look          looking       looks\n",
    "ltd           mainly        many          may           maybe\n",
    "me            mean          meanwhile     merely        might\n",
    "more          moreover      most          mostly        much\n",
    "must          my            myself        name          namely\n",
    "nd            near          nearly        necessary     need\n",
    "needs         neither       never         nevertheless  new\n",
    "next          nine          no            nobody        non\n",
    "none          noone         nor           normally      not\n",
    "nothing       novel         now           nowhere       obviously\n",
    "of            off           often         oh            ok\n",
    "okay          old           on            once          one\n",
    "ones          only          onto          or            other\n",
    "others        otherwise     ought         our           ours\n",
    "ourselves     out           outside       over          overall\n",
    "own           particular    particularly  per           perhaps\n",
    "placed        please        plus          possible      presumably\n",
    "probably      provides      que           quite         qv\n",
    "rather        rd            re            really        reasonably\n",
    "regarding     regardless    regards       relatively    respectively\n",
    "right         said          same          saw           say\n",
    "saying        says          second        secondly      see\n",
    "seeing        seem          seemed        seeming       seems\n",
    "seen          self          selves        sensible      sent\n",
    "serious       seriously     seven         several       shall\n",
    "she           should        shouldn't     since         six\n",
    "so            some          somebody      somehow       someone\n",
    "something     sometime      sometimes     somewhat      somewhere\n",
    "soon          sorry         specified     specify       specifying\n",
    "still         sub           such          sup           sure\n",
    "t's           take          taken         tell          tends\n",
    "th            than          thank         thanks        thanx\n",
    "that          that's        thats         the           their\n",
    "theirs        them          themselves    then          thence\n",
    "there         there's       thereafter    thereby       therefore\n",
    "therein       theres        thereupon     these         they\n",
    "they'd        they'll       they're       they've       think\n",
    "third         this          thorough      thoroughly    those\n",
    "though        three         through       throughout    thru\n",
    "thus          to            together      too           took\n",
    "toward        towards       tried         tries         truly\n",
    "try           trying        twice         two           un\n",
    "under         unfortunately unless        unlikely      until\n",
    "unto          up            upon          us            use\n",
    "used          useful        uses          using         usually\n",
    "value         various       very          via           viz\n",
    "vs            want          wants         was           wasn't\n",
    "way           we            we'd          we'll         we're\n",
    "we've         welcome       well          went          were\n",
    "weren't       what          what's        whatever      when\n",
    "whence        whenever      where         where's       whereafter\n",
    "whereas       whereby       wherein       whereupon     wherever\n",
    "whether       which         while         whither       who\n",
    "who's         whoever       whole         whom          whose\n",
    "why           will          willing       wish          with\n",
    "within        without       won't         wonder        would\n",
    "wouldn't      yes           yet           you           you'd\n",
    "you'll        you're        you've        your          yours\n",
    "yourself      yourselves    zero\"\"\"\n",
    "\n",
    "stopwords = stopwords.split()\n",
    "\n",
    "def wikitext_clean(wikitext):\n",
    "    curly_counter = 0\n",
    "    square_counter = 0\n",
    "\n",
    "    output_characters = ['']*len(text)\n",
    "    character_pointer = 0\n",
    "\n",
    "    link_record = []\n",
    "\n",
    "    for c in text:\n",
    "        if c == '{':\n",
    "            curly_counter += 1\n",
    "\n",
    "        if square_counter == 2:\n",
    "            link_record.append(c)\n",
    "\n",
    "        if curly_counter == 0 and c == '[':\n",
    "            square_counter += 1\n",
    "\n",
    "        if curly_counter == 0 and square_counter == 0:\n",
    "            output_characters[character_pointer] = c\n",
    "            character_pointer += 1\n",
    "\n",
    "        if c == '}':\n",
    "            curly_counter -= 1\n",
    "\n",
    "        if curly_counter == 0 and square_counter == 2 and c == '|':\n",
    "            link_record = []\n",
    "\n",
    "        if curly_counter == 0 and c == ']':\n",
    "            square_counter -= 1\n",
    "\n",
    "            if square_counter == 0:\n",
    "                output_characters[character_pointer] = \"~>\" + ''.join(link_record)[:-1] + \"<~\"\n",
    "                character_pointer += 1\n",
    "                link_record = []\n",
    "\n",
    "    return ''.join(output_characters)\n",
    "\n",
    "alphabet = list(\"0123456789\") + list(map(chr, range(97, 97+26)))\n",
    "alphabet = list(map(chr, range(97, 97+26)))\n",
    "\n",
    "print(f'alphabet: {\" \".join(alphabet)}')\n",
    "\n",
    "coordinates_word_index = {}\n",
    "titles_word_index = {}\n",
    "\n",
    "for index, (title, text) in enumerate(titles_texts):\n",
    "    cleaned = wikitext_clean(text).lower()\n",
    "\n",
    "    words = ''.join([c if c in alphabet else ' ' for c in cleaned]).split()\n",
    "    words_destopped = set(words).difference(stopwords)\n",
    "    words_deshorted = set(word for word in words_destopped if len(word) >= 3)\n",
    "\n",
    "    len(words), len(set(words)), len(words_destopped), len(words_deshorted)\n",
    "    words_deshorted\n",
    "\n",
    "    coordinate_indices = [coordinate_index(coordinate_text) for coordinate_text in coordinates_parse(text)\n",
    "                          if coordinate_index(coordinate_text)]\n",
    "\n",
    "    for word in words_deshorted:\n",
    "        coordinates_word_index[word] = coordinates_word_index.get(word, set())\n",
    "        coordinates_word_index[word].update(coordinate_indices)\n",
    "\n",
    "        titles_word_index[word] = titles_word_index.get(word, set())\n",
    "        titles_word_index[word].add(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('database/coordinates_master_list.json', 'w') as file:\n",
    "    file.write(json.dumps(coordinates_master_list, separators=(',', ':')))\n",
    "\n",
    "with open('database/coordinates_word_index.json', 'w') as file:\n",
    "    file.write(json.dumps({key: list(value) for key, value in coordinates_word_index.items()}, separators=(',', ':')))\n",
    "\n",
    "with open('database/titles_word_index.json', 'w') as file:\n",
    "    file.write(json.dumps({key: list(value) for key, value in titles_word_index.items()}, separators=(',', ':')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'api.php?action=query&list=search&srsearch=Craig%20Noone&format=json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "CPU times: user 98 µs, sys: 0 ns, total: 98 µs\nWall time: 102 µs\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(34.91916666666666, -115.06277777777777),\n (38.17027777777778, -119.1975),\n (50.72555555555556, -121.28055555555555),\n (38.1701984, 119.1973749),\n (42.468333333333334, -108.79972222222221),\n (44.464444444444446, -109.61361111111111),\n (35.0, 59.0),\n (44.397777777777776, -114.31166666666667),\n (20.0, 20.0),\n (4.3116, -52.137),\n (53.63249, -1.37556),\n (39.59388888888889, -121.52944444444444),\n (44.96666666666667, -78.25),\n (39.556666666666665, -121.4413888888889)]"
     },
     "metadata": {},
     "execution_count": 106
    }
   ],
   "source": [
    "%%time\n",
    "len(index.get('ghost')), len(index.get('town')), len(index['ghost'].intersection(index['town']).intersection(index['mining']))\n",
    "[coordinates_master_list[i] for i in index['ghost'].intersection(index['town']).intersection(index['mining'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1,\n       1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2,\n       2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0,\n       1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2,\n       2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0,\n       0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1,\n       2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0,\n       0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1,\n       1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2,\n       0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1,\n       1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2, 2, 0, 0, 0, 1, 1, 1, 2, 2,\n       2])"
     },
     "metadata": {},
     "execution_count": null
    }
   ],
   "source": [
    "(np.arange(3**5) / (3**1) % 3).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 2],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 1],\n",
       "       [0, 1, 2],\n",
       "       [0, 2, 0],\n",
       "       [0, 2, 1],\n",
       "       [0, 2, 2],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 1],\n",
       "       [1, 0, 2],\n",
       "       [1, 1, 0],\n",
       "       [1, 1, 1],\n",
       "       [1, 1, 2],\n",
       "       [1, 2, 0],\n",
       "       [1, 2, 1],\n",
       "       [1, 2, 2],\n",
       "       [2, 0, 0],\n",
       "       [2, 0, 1],\n",
       "       [2, 0, 2],\n",
       "       [2, 1, 0],\n",
       "       [2, 1, 1],\n",
       "       [2, 1, 2],\n",
       "       [2, 2, 0],\n",
       "       [2, 2, 1],\n",
       "       [2, 2, 2]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_power = 3\n",
    "indices = np.stack([(np.arange(3**max_power) / (3**power) % 3).astype(np.int) for power in range(3)]).transpose()[:,::-1]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = len(alphabet)\n",
    "max_power = 3\n",
    "\n",
    "indices = np.stack([(np.arange(base**max_power) / (base**power) % base).astype(np.int) for power in range(max_power)]).transpose()[:,::-1]\n",
    "\n",
    "code_to_tuple_list = list(map(''.join, np.array(alphabet)[indices].tolist()))\n",
    "tuple_to_code_dict = {tuple_: code for code, tuple_ in enumerate(code_to_tuple_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sequences = lambda w, n: [w[i:i+n] for i in range(len(w) - (n - 1))]\n",
    "\n",
    "eleven = {}\n",
    "for tuple_ in set(sum([n_sequences(word, 11) for word in words], [])):\n",
    "    eleven[tuple_] = eleven.get(tuple_, []) + [page_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'commemorati': [5],\n",
       " 'proximately': [5],\n",
       " 'dditionally': [5],\n",
       " 'additionall': [5],\n",
       " 'approximate': [5],\n",
       " 'admasambhav': [5],\n",
       " 'corporation': [5],\n",
       " 'pproximatel': [5],\n",
       " 'bodhisattva': [5],\n",
       " 'respectivel': [5],\n",
       " 'espectively': [5],\n",
       " 'mmemorating': [5],\n",
       " 'padmasambha': [5],\n",
       " 'overlooking': [5],\n",
       " 'ommemoratin': [5],\n",
       " 'celebrating': [5],\n",
       " 'anniversary': [5],\n",
       " 'constructed': [5],\n",
       " 'dmasambhava': [5],\n",
       " 'constructio': [5],\n",
       " 'onstruction': [5],\n",
       " 'accommodate': [5]}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eleven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('z1': conda)",
   "language": "python",
   "name": "python37764bitz1condae7f2c20f305643b296aa656a2776ff29"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}